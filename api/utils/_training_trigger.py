# -*- coding: utf-8 -*-
"""
è®­ç»ƒè§¦å‘å™¨æ¨¡å—
åœ¨åå°å¼‚æ­¥æ‰§è¡ŒMLæ¨¡å‹è®­ç»ƒä»»åŠ¡
"""
import os
import sys
import json
import pickle
import threading
import subprocess
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))
project_root = os.path.join(os.path.dirname(__file__), '../..')
sys.path.insert(0, project_root)

# è®­ç»ƒçŠ¶æ€æ–‡ä»¶
TRAINING_STATUS_FILE = '/tmp/training_status.json'
TRAINING_LOG_FILE = '/tmp/training_log.txt'


def get_training_status() -> Dict[str, Any]:
    """è·å–å½“å‰è®­ç»ƒçŠ¶æ€"""
    if os.path.exists(TRAINING_STATUS_FILE):
        try:
            with open(TRAINING_STATUS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass

    return {
        'status': 'idle',
        'message': 'æœªè¿è¡Œ',
        'started_at': None,
        'completed_at': None
    }


def update_training_status(status: str, message: str, **kwargs):
    """æ›´æ–°è®­ç»ƒçŠ¶æ€"""
    status_data = {
        'status': status,
        'message': message,
        'updated_at': datetime.now().isoformat(),
        **kwargs
    }

    try:
        with open(TRAINING_STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸  æ›´æ–°çŠ¶æ€å¤±è´¥: {str(e)}")


def prepare_training_data_from_combined(combined_data: List[Dict[str, Any]]) -> bool:
    """
    ä»åˆå¹¶çš„æ•°æ®å‡†å¤‡è®­ç»ƒæ•°æ®

    Args:
        combined_data: åˆå¹¶åçš„å®Œæ•´æ•°æ®

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        print("ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        training_dir = os.path.join(project_root, 'data', 'training')
        os.makedirs(training_dir, exist_ok=True)

        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼ˆç®€åŒ–ç‰ˆç‰¹å¾æå–ï¼‰
        X_data = []
        y_front_data = []
        y_back_data = []

        # ä½¿ç”¨æ»‘åŠ¨çª—å£æå–ç‰¹å¾ï¼ˆè¿‡å»10æœŸé¢„æµ‹ä¸‹ä¸€æœŸï¼‰
        window_size = 10

        for i in range(len(combined_data) - window_size):
            # æå–è¿‡å»window_sizeæœŸçš„æ•°æ®ä½œä¸ºç‰¹å¾
            history_window = combined_data[i:i+window_size]

            # ç®€å•ç‰¹å¾ï¼šå±•å¹³æ‰€æœ‰å·ç 
            features = []
            for record in history_window:
                features.extend(record.get('front_zone', []))
                features.extend(record.get('back_zone', []))

            # ç›®æ ‡ï¼šä¸‹ä¸€æœŸçš„å·ç ï¼ˆone-hotç¼–ç ï¼‰
            next_record = combined_data[i + window_size]

            # å‰åŒºone-hotï¼ˆ35ä¸ªå·ç ï¼‰
            front_target = [0] * 35
            for num in next_record.get('front_zone', []):
                if 1 <= num <= 35:
                    front_target[num - 1] = 1

            # ååŒºone-hotï¼ˆ12ä¸ªå·ç ï¼‰
            back_target = [0] * 12
            for num in next_record.get('back_zone', []):
                if 1 <= num <= 12:
                    back_target[num - 1] = 1

            X_data.append(features)
            y_front_data.append(front_target)
            y_back_data.append(back_target)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        import numpy as np
        X = np.array(X_data)
        y_front = np.array(y_front_data)
        y_back = np.array(y_back_data)

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ80/20ï¼‰
        split_idx = int(len(X) * 0.8)

        training_data = {
            'X_train': X[:split_idx],
            'X_test': X[split_idx:],
            'y_front_train': y_front[:split_idx],
            'y_front_test': y_front[split_idx:],
            'y_back_train': y_back[:split_idx],
            'y_back_test': y_back[split_idx:],
            'created_at': datetime.now().isoformat(),
            'total_samples': len(X),
            'train_samples': split_idx,
            'test_samples': len(X) - split_idx
        }

        # ä¿å­˜è®­ç»ƒæ•°æ®
        training_file = os.path.join(training_dir, 'training_data.pkl')
        with open(training_file, 'wb') as f:
            pickle.dump(training_data, f)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'total_samples': training_data['total_samples'],
            'train_samples': training_data['train_samples'],
            'test_samples': training_data['test_samples'],
            'feature_dim': X.shape[1],
            'created_at': training_data['created_at'],
            'data_source': 'combined_user_and_fixed'
        }

        metadata_file = os.path.join(training_dir, 'metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"   æ€»æ ·æœ¬æ•°: {training_data['total_samples']}")
        print(f"   è®­ç»ƒé›†: {training_data['train_samples']}")
        print(f"   æµ‹è¯•é›†: {training_data['test_samples']}")

        return True

    except Exception as e:
        print(f"âŒ å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def run_training_script() -> bool:
    """
    è¿è¡Œè®­ç»ƒè„šæœ¬

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        print("ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹...")

        # è®­ç»ƒè„šæœ¬è·¯å¾„
        script_path = os.path.join(project_root, 'scripts', 'train_models.py')

        if not os.path.exists(script_path):
            print(f"âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {script_path}")
            return False

        # æ‰§è¡Œè®­ç»ƒè„šæœ¬ï¼Œè¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
        with open(TRAINING_LOG_FILE, 'w', encoding='utf-8') as log_file:
            log_file.write(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().isoformat()}\n")
            log_file.write("="*60 + "\n\n")

            result = subprocess.run(
                [sys.executable, script_path],
                cwd=project_root,
                stdout=log_file,
                stderr=subprocess.STDOUT,
                text=True
            )

            log_file.write(f"\n\nè®­ç»ƒç»“æŸæ—¶é—´: {datetime.now().isoformat()}\n")
            log_file.write(f"é€€å‡ºä»£ç : {result.returncode}\n")

        if result.returncode == 0:
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return True
        else:
            print(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œé€€å‡ºä»£ç : {result.returncode}")
            print(f"   æŸ¥çœ‹æ—¥å¿—: {TRAINING_LOG_FILE}")
            return False

    except Exception as e:
        print(f"âŒ è®­ç»ƒè„šæœ¬æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def upload_models_to_cos() -> bool:
    """
    ä¸Šä¼ è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°COS

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        # æ£€æŸ¥COSé…ç½®
        if not all([
            os.getenv('TENCENT_SECRET_ID'),
            os.getenv('TENCENT_SECRET_KEY'),
            os.getenv('TENCENT_COS_BUCKET')
        ]):
            print("âš ï¸  è…¾è®¯äº‘COSæœªé…ç½®ï¼Œè·³è¿‡æ¨¡å‹ä¸Šä¼ ")
            return False

        print("ğŸ“¤ ä¸Šä¼ æ¨¡å‹åˆ°è…¾è®¯äº‘COS...")

        from tencent_cos import get_cos_client

        client = get_cos_client()
        models_dir = os.path.join(project_root, 'data', 'models')

        if not os.path.exists(models_dir):
            print(f"âš ï¸  æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
            return False

        # ä¸Šä¼ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = [f for f in os.listdir(models_dir) if f.endswith(('.pkl', '.h5', '.json'))]

        if not model_files:
            print("âš ï¸  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            return False

        success_count = 0
        for filename in model_files:
            local_path = os.path.join(models_dir, filename)
            cos_path = f'models/{filename}'

            result = client.upload_file(local_path, cos_path)
            if result['success']:
                success_count += 1

        print(f"âœ… æˆåŠŸä¸Šä¼  {success_count}/{len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")
        return success_count > 0

    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸Šä¼ å¤±è´¥: {str(e)}")
        return False


def training_task(combined_data: List[Dict[str, Any]]):
    """
    åå°è®­ç»ƒä»»åŠ¡ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼‰

    Args:
        combined_data: åˆå¹¶åçš„å®Œæ•´æ•°æ®
    """
    print("\n" + "="*70)
    print("ğŸš€ åå°è®­ç»ƒä»»åŠ¡å¯åŠ¨")
    print("="*70)

    update_training_status(
        'running',
        'æ­£åœ¨è®­ç»ƒæ¨¡å‹...',
        started_at=datetime.now().isoformat()
    )

    try:
        # æ­¥éª¤1ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
        print("\nğŸ“Š æ­¥éª¤1ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®")
        if not prepare_training_data_from_combined(combined_data):
            update_training_status('failed', 'å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥')
            return

        # æ­¥éª¤2ï¼šè¿è¡Œè®­ç»ƒè„šæœ¬
        print("\nğŸ¯ æ­¥éª¤2ï¼šè®­ç»ƒæ¨¡å‹")
        if not run_training_script():
            update_training_status('failed', 'æ¨¡å‹è®­ç»ƒå¤±è´¥')
            return

        # æ­¥éª¤3ï¼šä¸Šä¼ æ¨¡å‹åˆ°COS
        print("\nâ˜ï¸  æ­¥éª¤3ï¼šä¸Šä¼ æ¨¡å‹åˆ°COS")
        upload_models_to_cos()  # ä¸å¼ºåˆ¶è¦æ±‚æˆåŠŸ

        # å®Œæˆ
        update_training_status(
            'completed',
            'æ¨¡å‹è®­ç»ƒå®Œæˆ',
            completed_at=datetime.now().isoformat()
        )

        print("\n" + "="*70)
        print("âœ… åå°è®­ç»ƒä»»åŠ¡å®Œæˆ")
        print("="*70)

    except Exception as e:
        error_msg = f'è®­ç»ƒä»»åŠ¡å¼‚å¸¸: {str(e)}'
        print(f"\nâŒ {error_msg}")
        update_training_status('failed', error_msg)
        import traceback
        traceback.print_exc()


def trigger_training(combined_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    è§¦å‘åå°è®­ç»ƒä»»åŠ¡

    Args:
        combined_data: åˆå¹¶åçš„å®Œæ•´æ•°æ®

    Returns:
        è§¦å‘ç»“æœ
    """
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒä»»åŠ¡åœ¨è¿è¡Œ
    current_status = get_training_status()

    if current_status.get('status') == 'running':
        return {
            'success': False,
            'message': 'å·²æœ‰è®­ç»ƒä»»åŠ¡æ­£åœ¨è¿è¡Œ',
            'status': current_status
        }

    # å¯åŠ¨åå°è®­ç»ƒçº¿ç¨‹
    thread = threading.Thread(
        target=training_task,
        args=(combined_data,),
        daemon=True
    )
    thread.start()

    return {
        'success': True,
        'message': 'åå°è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨',
        'status': 'running',
        'log_file': TRAINING_LOG_FILE,
        'status_file': TRAINING_STATUS_FILE
    }


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    print("="*60)
    print("æµ‹è¯•è®­ç»ƒè§¦å‘å™¨")
    print("="*60)

    # åŠ è½½æµ‹è¯•æ•°æ®
    from _lottery_data import lottery_data

    # è§¦å‘è®­ç»ƒ
    result = trigger_training(lottery_data)

    print(f"\nè§¦å‘ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")

    # ç­‰å¾…ä¸€ä¼šå„¿æŸ¥çœ‹çŠ¶æ€
    import time
    time.sleep(5)

    status = get_training_status()
    print(f"\nå½“å‰çŠ¶æ€: {json.dumps(status, ensure_ascii=False, indent=2)}")
