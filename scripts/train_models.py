"""
è®­ç»ƒ4ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹å¹¶è½¬æ¢ä¸ºVercelå…¼å®¹æ ¼å¼
1. Random Forest (.pkl)
2. XGBoost (.pkl)
3. LSTM (.onnx)
4. Transformer (.onnx)

è¿è¡Œå‰å®‰è£…ä¾èµ–:
pip install scikit-learn xgboost tensorflow tf2onnx onnxruntime
"""
import sys
import os
import pickle
import json
import numpy as np
from datetime import datetime

# ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import hamming_loss

# XGBoost
import xgboost as xgb

# æ·±åº¦å­¦ä¹ 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


def load_training_data(data_dir='data/training'):
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    try:
        with open(f'{data_dir}/training_data.pkl', 'rb') as f:
            data = pickle.load(f)
        return data
    except FileNotFoundError:
        print("âš ï¸  è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½®æ•°æ®ç”Ÿæˆ...")
        return generate_training_data()


def generate_training_data():
    """ä»lottery_dataç”Ÿæˆè®­ç»ƒæ•°æ®"""
    # å°è¯•åŠ è½½æœ¬åœ°æ•°æ®
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'api'))

    try:
        from utils._lottery_data import lottery_data
        data = lottery_data
    except ImportError:
        print("âŒ æ— æ³•åŠ è½½lottery_data")
        sys.exit(1)

    print(f"ğŸ“Š ä½¿ç”¨ {len(data)} æœŸæ•°æ®ç”Ÿæˆè®­ç»ƒé›†...")

    sequence_length = 10
    X = []
    y_front = []
    y_back = []

    for i in range(len(data) - sequence_length):
        # ç‰¹å¾ï¼šè¿‡å»sequence_lengthæœŸçš„å·ç 
        features = []
        for j in range(sequence_length):
            record = data[i + j]
            front = record.get('front_zone', [])
            back = record.get('back_zone', [])
            features.extend(front[:5] if len(front) >= 5 else front + [0] * (5 - len(front)))
            features.extend(back[:2] if len(back) >= 2 else back + [0] * (2 - len(back)))

        # æ ‡ç­¾ï¼šä¸‹ä¸€æœŸçš„å·ç 
        next_record = data[i + sequence_length]
        next_front = next_record.get('front_zone', [])
        next_back = next_record.get('back_zone', [])

        # å‰åŒºæ ‡ç­¾ (35ç»´one-hot)
        front_label = np.zeros(35)
        for n in next_front[:5]:
            if 1 <= n <= 35:
                front_label[n - 1] = 1

        # ååŒºæ ‡ç­¾ (12ç»´one-hot)
        back_label = np.zeros(12)
        for n in next_back[:2]:
            if 1 <= n <= 12:
                back_label[n - 1] = 1

        X.append(features)
        y_front.append(front_label)
        y_back.append(back_label)

    X = np.array(X, dtype=np.float32)
    y_front = np.array(y_front, dtype=np.float32)
    y_back = np.array(y_back, dtype=np.float32)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    split = int(len(X) * 0.8)

    return {
        'X_train': X[:split],
        'X_test': X[split:],
        'y_front_train': y_front[:split],
        'y_front_test': y_front[split:],
        'y_back_train': y_back[:split],
        'y_back_test': y_back[split:]
    }


def train_random_forest(X_train, y_train, X_test, y_test, zone='front'):
    """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ - {zone}åŒº")
    print(f"{'='*60}")

    model = MultiOutputClassifier(
        RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
    )

    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    model.fit(X_train, y_train)

    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)

    train_loss = hamming_loss(y_train, train_pred)
    test_loss = hamming_loss(y_test, test_pred)

    print(f"âœ… è®­ç»ƒå®Œæˆ")
    print(f"   è®­ç»ƒé›† Hamming Loss: {train_loss:.4f}")
    print(f"   æµ‹è¯•é›† Hamming Loss: {test_loss:.4f}")

    return model, {
        'train_loss': float(train_loss),
        'test_loss': float(test_loss),
        'model_type': 'RandomForest',
        'zone': zone
    }


def train_xgboost(X_train, y_train, X_test, y_test, zone='front'):
    """è®­ç»ƒXGBoostæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒXGBoostæ¨¡å‹ - {zone}åŒº")
    print(f"{'='*60}")

    models = []
    n_outputs = y_train.shape[1]

    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ {n_outputs} ä¸ªè¾“å‡º...")

    for i in range(n_outputs):
        if (i + 1) % 10 == 0:
            print(f"   è¿›åº¦: {i+1}/{n_outputs}")

        clf = xgb.XGBClassifier(
            n_estimators=50,
            max_depth=5,
            learning_rate=0.1,
            random_state=42,
            tree_method='hist',
            use_label_encoder=False,
            eval_metric='logloss'
        )
        clf.fit(X_train, y_train[:, i])
        models.append(clf)

    train_preds = np.array([m.predict(X_train) for m in models]).T
    test_preds = np.array([m.predict(X_test) for m in models]).T

    train_loss = hamming_loss(y_train, train_preds)
    test_loss = hamming_loss(y_test, test_preds)

    print(f"âœ… è®­ç»ƒå®Œæˆ")
    print(f"   è®­ç»ƒé›† Hamming Loss: {train_loss:.4f}")
    print(f"   æµ‹è¯•é›† Hamming Loss: {test_loss:.4f}")

    return models, {
        'train_loss': float(train_loss),
        'test_loss': float(test_loss),
        'model_type': 'XGBoost',
        'zone': zone,
        'n_models': len(models)
    }


def create_lstm_model(sequence_length, feature_dim, output_dim):
    """åˆ›å»ºLSTMæ¨¡å‹"""
    model = keras.Sequential([
        layers.Input(shape=(sequence_length, feature_dim)),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(32),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        layers.Dense(output_dim, activation='sigmoid')
    ])

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model


def train_lstm(X_train, y_train, X_test, y_test, zone='front'):
    """è®­ç»ƒLSTMæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒLSTMæ¨¡å‹ - {zone}åŒº")
    print(f"{'='*60}")

    # é‡å¡‘æ•°æ®ä¸ºæ—¶é—´åºåˆ—æ ¼å¼ (samples, timesteps, features)
    n_timesteps = 10
    feature_dim = X_train.shape[1] // n_timesteps

    X_train_seq = X_train[:, :n_timesteps * feature_dim].reshape(-1, n_timesteps, feature_dim)
    X_test_seq = X_test[:, :n_timesteps * feature_dim].reshape(-1, n_timesteps, feature_dim)

    model = create_lstm_model(n_timesteps, feature_dim, y_train.shape[1])

    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    history = model.fit(
        X_train_seq, y_train,
        validation_data=(X_test_seq, y_test),
        epochs=30,
        batch_size=32,
        verbose=1
    )

    train_loss = history.history['loss'][-1]
    test_loss = history.history['val_loss'][-1]

    print(f"âœ… è®­ç»ƒå®Œæˆ")
    print(f"   è®­ç»ƒé›† Loss: {train_loss:.4f}")
    print(f"   æµ‹è¯•é›† Loss: {test_loss:.4f}")

    return model, {
        'train_loss': float(train_loss),
        'test_loss': float(test_loss),
        'model_type': 'LSTM',
        'zone': zone,
        'input_shape': (n_timesteps, feature_dim)
    }


def create_transformer_model(input_dim, output_dim):
    """åˆ›å»ºTransformeræ¨¡å‹"""
    inputs = layers.Input(shape=(input_dim,))

    # é‡å¡‘ä¸ºåºåˆ—
    x = layers.Dense(128)(inputs)
    x = layers.Reshape((16, 8))(x)

    # Multi-head attention
    attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=8)(x, x)
    x = layers.Add()([x, attention_output])
    x = layers.LayerNormalization()(x)

    # Feed forward
    ff = layers.Dense(64, activation='relu')(x)
    ff = layers.Dense(8)(ff)
    x = layers.Add()([x, ff])
    x = layers.LayerNormalization()(x)

    # è¾“å‡ºå±‚
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(output_dim, activation='sigmoid')(x)

    model = keras.Model(inputs=inputs, outputs=outputs)

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model


def train_transformer(X_train, y_train, X_test, y_test, zone='front'):
    """è®­ç»ƒTransformeræ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒTransformeræ¨¡å‹ - {zone}åŒº")
    print(f"{'='*60}")

    model = create_transformer_model(X_train.shape[1], y_train.shape[1])

    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=30,
        batch_size=32,
        verbose=1
    )

    train_loss = history.history['loss'][-1]
    test_loss = history.history['val_loss'][-1]

    print(f"âœ… è®­ç»ƒå®Œæˆ")
    print(f"   è®­ç»ƒé›† Loss: {train_loss:.4f}")
    print(f"   æµ‹è¯•é›† Loss: {test_loss:.4f}")

    return model, {
        'train_loss': float(train_loss),
        'test_loss': float(test_loss),
        'model_type': 'Transformer',
        'zone': zone,
        'input_shape': (X_train.shape[1],)
    }


def convert_keras_to_onnx(model, model_name, input_shape, output_dir):
    """å°†Kerasæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼"""
    import tf2onnx

    print(f"\nğŸ”„ è½¬æ¢ {model_name} åˆ°ONNXæ ¼å¼...")

    output_path = f'{output_dir}/{model_name}.onnx'

    # å®šä¹‰è¾“å…¥ç­¾å
    spec = (tf.TensorSpec(input_shape, tf.float32, name="input"),)

    # è½¬æ¢
    model_proto, _ = tf2onnx.convert.from_keras(
        model,
        input_signature=spec,
        output_path=output_path
    )

    print(f"âœ… å·²ä¿å­˜: {output_path}")
    return output_path


def save_models(models, output_dir='models'):
    """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
    os.makedirs(output_dir, exist_ok=True)

    saved_info = {'models': {}}

    for model_name, (model, metadata) in models.items():
        if 'lstm' in model_name.lower():
            # LSTMæ¨¡å‹è½¬æ¢ä¸ºONNX
            input_shape = metadata.get('input_shape', (10, 7))
            batch_input_shape = (None,) + input_shape
            model_path = convert_keras_to_onnx(model, model_name, batch_input_shape, output_dir)
            saved_info['models'][model_name] = {
                'type': 'onnx',
                'format': 'onnx',
                'path': model_path,
                'metadata': metadata
            }

        elif 'transformer' in model_name.lower():
            # Transformeræ¨¡å‹è½¬æ¢ä¸ºONNX
            input_shape = metadata.get('input_shape', (70,))
            batch_input_shape = (None,) + input_shape
            model_path = convert_keras_to_onnx(model, model_name, batch_input_shape, output_dir)
            saved_info['models'][model_name] = {
                'type': 'onnx',
                'format': 'onnx',
                'path': model_path,
                'metadata': metadata
            }

        else:
            # sklearnå’Œxgboostæ¨¡å‹ä¿å­˜ä¸º.pkl
            model_path = f'{output_dir}/{model_name}.pkl'
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            saved_info['models'][model_name] = {
                'type': 'sklearn',
                'format': 'pkl',
                'path': model_path,
                'metadata': metadata
            }

        print(f"ğŸ’¾ å·²ä¿å­˜: {model_name}")

    # ä¿å­˜æ¨¡å‹ä¿¡æ¯
    saved_info['version'] = '2.0.0'
    saved_info['trained_at'] = datetime.now().isoformat()

    info_path = f'{output_dir}/models_info.json'
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(saved_info, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_path}")
    return saved_info


def upload_to_cos(output_dir='models'):
    """ä¸Šä¼ æ¨¡å‹åˆ°è…¾è®¯äº‘COS"""
    # æ£€æŸ¥COSé…ç½®
    cos_configured = all([
        os.getenv('TENCENT_SECRET_ID'),
        os.getenv('TENCENT_SECRET_KEY'),
        os.getenv('TENCENT_COS_BUCKET'),
        os.getenv('TENCENT_COS_REGION')
    ])

    if not cos_configured:
        print("\nâš ï¸  è…¾è®¯äº‘COSæœªé…ç½®ï¼Œè·³è¿‡ä¸Šä¼ ")
        print("   è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        print("   - TENCENT_SECRET_ID")
        print("   - TENCENT_SECRET_KEY")
        print("   - TENCENT_COS_BUCKET")
        print("   - TENCENT_COS_REGION")
        return

    print("\nğŸ“¤ ä¸Šä¼ æ¨¡å‹åˆ°è…¾è®¯äº‘COS...")

    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'api'))
    from utils.tencent_cos import get_cos_client

    client = get_cos_client()

    # ä¸Šä¼ æ‰€æœ‰æ¨¡å‹æ–‡ä»¶
    for filename in os.listdir(output_dir):
        local_path = os.path.join(output_dir, filename)
        cos_path = f'models/{filename}'

        print(f"   ä¸Šä¼ : {filename} -> {cos_path}")
        client.upload_file(local_path, cos_path)

    print("âœ… æ‰€æœ‰æ¨¡å‹å·²ä¸Šä¼ åˆ°COS")


def main():
    print("=" * 70)
    print("ğŸ¤– è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ (Vercelå…¼å®¹ç‰ˆ)")
    print("=" * 70)

    # åŠ è½½æ•°æ®
    print("\nğŸ“š åŠ è½½è®­ç»ƒæ•°æ®...")
    data = load_training_data()

    X_train = data['X_train']
    X_test = data['X_test']
    y_front_train = data['y_front_train']
    y_front_test = data['y_front_test']
    y_back_train = data['y_back_train']
    y_back_test = data['y_back_test']

    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    print(f"   ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
    print(f"   å‰åŒºè¾“å‡º: {y_front_train.shape[1]}")
    print(f"   ååŒºè¾“å‡º: {y_back_train.shape[1]}")

    models = {}

    # è®­ç»ƒå‰åŒºæ¨¡å‹
    print("\n" + "=" * 70)
    print("è®­ç»ƒå‰åŒºæ¨¡å‹")
    print("=" * 70)

    rf_front, rf_front_meta = train_random_forest(
        X_train, y_front_train, X_test, y_front_test, 'front'
    )
    models['random_forest_front'] = (rf_front, rf_front_meta)

    xgb_front, xgb_front_meta = train_xgboost(
        X_train, y_front_train, X_test, y_front_test, 'front'
    )
    models['xgboost_front'] = (xgb_front, xgb_front_meta)

    lstm_front, lstm_front_meta = train_lstm(
        X_train, y_front_train, X_test, y_front_test, 'front'
    )
    models['lstm_front'] = (lstm_front, lstm_front_meta)

    transformer_front, transformer_front_meta = train_transformer(
        X_train, y_front_train, X_test, y_front_test, 'front'
    )
    models['transformer_front'] = (transformer_front, transformer_front_meta)

    # è®­ç»ƒååŒºæ¨¡å‹
    print("\n" + "=" * 70)
    print("è®­ç»ƒååŒºæ¨¡å‹")
    print("=" * 70)

    rf_back, rf_back_meta = train_random_forest(
        X_train, y_back_train, X_test, y_back_test, 'back'
    )
    models['random_forest_back'] = (rf_back, rf_back_meta)

    xgb_back, xgb_back_meta = train_xgboost(
        X_train, y_back_train, X_test, y_back_test, 'back'
    )
    models['xgboost_back'] = (xgb_back, xgb_back_meta)

    lstm_back, lstm_back_meta = train_lstm(
        X_train, y_back_train, X_test, y_back_test, 'back'
    )
    models['lstm_back'] = (lstm_back, lstm_back_meta)

    transformer_back, transformer_back_meta = train_transformer(
        X_train, y_back_train, X_test, y_back_test, 'back'
    )
    models['transformer_back'] = (transformer_back, transformer_back_meta)

    # ä¿å­˜æ¨¡å‹
    print(f"\n{'='*70}")
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹ (sklearn -> .pkl, keras -> .onnx)")
    print(f"{'='*70}")

    saved_info = save_models(models, output_dir='models')

    # ä¸Šä¼ åˆ°COS
    upload_to_cos(output_dir='models')

    print(f"\n{'='*70}")
    print("âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*70}")
    print(f"\nå…±è®­ç»ƒäº† {len(models)} ä¸ªæ¨¡å‹ï¼š")
    for name, info in saved_info['models'].items():
        meta = info['metadata']
        print(f"  - {name} ({info['format']}): æµ‹è¯•Loss={meta['test_loss']:.4f}")

    print("\nğŸ“ æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: models/")
    print("   - *.pkl: sklearn/xgboostæ¨¡å‹ (Vercelå¯ç›´æ¥åŠ è½½)")
    print("   - *.onnx: LSTM/Transformeræ¨¡å‹ (éœ€è¦onnxruntime)")

    return models, saved_info


if __name__ == '__main__':
    main()
